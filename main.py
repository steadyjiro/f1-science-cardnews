"""
F1 Science Card News â€” ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸
GitHub Actionsì—ì„œ ìë™ ì‹¤í–‰ë¨. ìˆ˜ë™ ê°œì… ë¶ˆí•„ìš”.
"""

import os
import sys
import json
import time
import re
import traceback
from datetime import datetime
from pathlib import Path

import requests
import pdfplumber

# â”€â”€ ì„¤ì • â”€â”€
GEMINI_KEY = os.environ.get("GEMINI_API_KEY", "")
GROQ_KEY = os.environ.get("GROQ_API_KEY", "")
PEXELS_KEY = os.environ.get("PEXELS_API_KEY", "")
SS_KEY = os.environ.get("SEMANTIC_SCHOLAR_API_KEY", "")

DATA_DIR = Path("data")
OUTPUT_DIR = Path("output")
TEMPLATES_DIR = Path("templates")
HISTORY_FILE = DATA_DIR / "processed_papers.json"
QUERIES_FILE = DATA_DIR / "queries.json"

from prompts import PROMPT_ANALYSIS, PROMPT_CARDNEWS, PROMPT_VERIFY


# =============================================
# STEP 1: ë…¼ë¬¸ ê²€ìƒ‰ (Semantic Scholar API)
# =============================================
def search_papers():
    """Semantic Scholarì—ì„œ F1 ìƒë¦¬í•™ ê´€ë ¨ OA ë…¼ë¬¸ ê²€ìƒ‰"""
    queries = json.loads(QUERIES_FILE.read_text())
    history = json.loads(HISTORY_FILE.read_text()) if HISTORY_FILE.exists() else []

    headers = {}
    if SS_KEY:
        headers["x-api-key"] = SS_KEY

    new_papers = []
    seen_dois = set(history)

    for query in queries:
        try:
            url = "https://api.semanticscholar.org/graph/v1/paper/search"
            params = {
                "query": query,
                "limit": 10,
                "fields": "title,authors,year,venue,externalIds,openAccessPdf,abstract,citationCount",
                "openAccessPdf": "",
                "year": "2015-",
            }
            resp = requests.get(url, params=params, headers=headers, timeout=15)

            if resp.status_code == 429:
                print(f"[WARN] Rate limited on query: {query}. Waiting 30s...")
                time.sleep(30)
                continue
            if resp.status_code != 200:
                print(f"[WARN] Search failed for '{query}': HTTP {resp.status_code}")
                continue

            data = resp.json()
            for paper in data.get("data", []):
                doi = (paper.get("externalIds") or {}).get("DOI")
                oa_pdf = paper.get("openAccessPdf")
                if doi and oa_pdf and doi not in seen_dois:
                    paper["doi"] = doi
                    paper["pdf_url"] = oa_pdf.get("url", "")
                    new_papers.append(paper)
                    seen_dois.add(doi)

            time.sleep(1.5)  # Rate limit: 1 req/sec for unauthenticated

        except Exception as e:
            print(f"[WARN] Search error for '{query}': {e}")
            continue

    # ì¸ìš© ìˆ˜ ê¸°ì¤€ ì •ë ¬, ìµœëŒ€ 2í¸ ì²˜ë¦¬
    new_papers.sort(key=lambda p: p.get("citationCount", 0), reverse=True)
    selected = new_papers[:2]

    if selected:
        print(f"âœ… Found {len(new_papers)} new papers, selected top {len(selected)}:")
        for p in selected:
            print(f"   - {p.get('title', 'Unknown')} (DOI: {p['doi']})")
    else:
        print("â„¹ï¸ No new papers found.")

    return selected


# =============================================
# STEP 2: PDF ë‹¤ìš´ë¡œë“œ + í…ìŠ¤íŠ¸ ì¶”ì¶œ
# =============================================
def download_and_extract(paper):
    """PDF ë‹¤ìš´ë¡œë“œ â†’ í…ìŠ¤íŠ¸ + Figure ì¶”ì¶œ"""
    pdf_url = paper.get("pdf_url", "")
    abstract = paper.get("abstract", "") or ""
    text = ""
    figures = []
    pdf_path = "/tmp/paper.pdf"

    if pdf_url:
        try:
            print(f"   Downloading PDF: {pdf_url[:80]}...")
            resp = requests.get(pdf_url, timeout=45, headers={
                "User-Agent": "F1ScienceCardNews/1.0 (research automation)"
            })
            if resp.status_code == 200 and len(resp.content) > 1000:
                with open(pdf_path, "wb") as f:
                    f.write(resp.content)

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (pdfplumber)
                with pdfplumber.open(pdf_path) as pdf:
                    pages_text = []
                    for page in pdf.pages[:25]:  # ìµœëŒ€ 25í˜ì´ì§€
                        pt = page.extract_text()
                        if pt:
                            pages_text.append(pt)
                    text = "\n".join(pages_text)

                # Figure ì¶”ì¶œ (PyMuPDF)
                figures = extract_figures_from_pdf(pdf_path)

            else:
                print(f"   [WARN] PDF download failed: HTTP {resp.status_code}")
        except Exception as e:
            print(f"   [WARN] PDF processing error: {e}")

    # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ abstractë¡œ í´ë°±
    if not text.strip():
        text = abstract
        print("   Using abstract only (PDF text extraction failed)")

    # í† í° ì ˆì•½: ìµœëŒ€ 15000ì
    if len(text) > 15000:
        text = text[:15000]

    return text, figures, pdf_path


def extract_figures_from_pdf(pdf_path):
    """PDFì—ì„œ Figure í›„ë³´ ì´ë¯¸ì§€ ì¶”ì¶œ (PyMuPDF)"""
    figures = []
    try:
        import fitz
        doc = fitz.open(pdf_path)
        fig_dir = Path("/tmp/figures")
        fig_dir.mkdir(exist_ok=True)

        for page_num in range(min(len(doc), 25)):
            page = doc[page_num]
            for img_idx, img in enumerate(page.get_images(full=True)):
                try:
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    if pix.n >= 5:  # CMYK â†’ RGB
                        pix = fitz.Pixmap(fitz.csRGB, pix)
                    # í¬ê¸° í•„í„°: 300x200 ì´ìƒë§Œ
                    if pix.width >= 300 and pix.height >= 200:
                        fname = f"figure_{page_num}_{img_idx}.png"
                        pix.save(str(fig_dir / fname))
                        figures.append({
                            "filename": fname,
                            "width": pix.width,
                            "height": pix.height,
                            "page": page_num
                        })
                except Exception:
                    continue
        doc.close()
    except ImportError:
        print("   [INFO] PyMuPDF not available, skipping figure extraction")
    except Exception as e:
        print(f"   [WARN] Figure extraction error: {e}")

    print(f"   Extracted {len(figures)} figure candidates")
    return figures


# =============================================
# STEP 3-5: LLM API í˜¸ì¶œ (í´ë°± ì²´ì¸)
# =============================================
def call_llm(prompt):
    """Gemini â†’ Groq â†’ Gemini Flash-Lite í´ë°± ì²´ì¸"""
    providers = []

    if GEMINI_KEY:
        providers.append(("gemini", "gemini-2.5-flash-preview-05-20", GEMINI_KEY))
    if GROQ_KEY:
        providers.append(("groq", "llama-3.3-70b-versatile", GROQ_KEY))
    if GEMINI_KEY:
        providers.append(("gemini", "gemini-2.0-flash-lite", GEMINI_KEY))

    if not providers:
        raise Exception("No LLM API keys configured!")

    for provider_type, model, key in providers:
        try:
            if provider_type == "gemini":
                return call_gemini(prompt, key, model)
            elif provider_type == "groq":
                return call_groq(prompt, key, model)
        except Exception as e:
            print(f"   [WARN] {model} failed: {e}")
            time.sleep(5)
            continue

    raise Exception("All LLM providers failed!")


def call_gemini(prompt, api_key, model="gemini-2.5-flash-preview-05-20"):
    """Google Gemini API í˜¸ì¶œ"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.3, "maxOutputTokens": 4096}
    }
    resp = requests.post(url, json=payload, timeout=60)

    if resp.status_code == 429:
        raise Exception("Gemini rate limited (429)")
    if resp.status_code != 200:
        raise Exception(f"Gemini HTTP {resp.status_code}: {resp.text[:200]}")

    data = resp.json()
    text = data["candidates"][0]["content"]["parts"][0]["text"]
    return text


def call_groq(prompt, api_key, model="llama-3.3-70b-versatile"):
    """GroqCloud API í˜¸ì¶œ"""
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,
        "max_tokens": 4096,
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=60)

    if resp.status_code == 429:
        raise Exception("Groq rate limited (429)")
    if resp.status_code != 200:
        raise Exception(f"Groq HTTP {resp.status_code}: {resp.text[:200]}")

    data = resp.json()
    return data["choices"][0]["message"]["content"]


def parse_json_response(text):
    """LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ì½”ë“œë¸”ë¡ ì œê±° ë“±)"""
    text = text.strip()
    # Remove markdown code blocks
    if "```" in text:
        matches = re.findall(r'```(?:json)?\s*([\s\S]*?)```', text)
        if matches:
            text = matches[0].strip()
    # Try direct parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    # Find first { to last }
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1:
        try:
            return json.loads(text[start:end+1])
        except json.JSONDecodeError:
            pass
    raise ValueError(f"Failed to parse JSON from LLM response: {text[:200]}...")


# =============================================
# STEP 6: ë¹„ì£¼ì–¼ ì†Œì‹± + ì¹´ë“œ ì´ë¯¸ì§€ ë Œë”ë§
# =============================================
def fetch_pexels_photo(query, output_path):
    """Pexels APIì—ì„œ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ"""
    if not PEXELS_KEY:
        print(f"   [WARN] No Pexels API key, skipping photo for: {query}")
        return None

    try:
        headers = {"Authorization": PEXELS_KEY}
        resp = requests.get(
            "https://api.pexels.com/v1/search",
            params={"query": query, "per_page": 5, "orientation": "square"},
            headers=headers, timeout=10
        )
        if resp.status_code == 200:
            photos = resp.json().get("photos", [])
            if photos:
                # ì²« ë²ˆì§¸ ì‚¬ì§„ì˜ ê³ í•´ìƒë„ ë²„ì „
                img_url = photos[0]["src"]["large2x"]
                img_resp = requests.get(img_url, timeout=15)
                if img_resp.status_code == 200:
                    with open(output_path, "wb") as f:
                        f.write(img_resp.content)
                    photographer = photos[0].get("photographer", "Unknown")
                    print(f"   ğŸ“¸ Pexels photo saved: {query} (by {photographer})")
                    return output_path
        print(f"   [WARN] Pexels search returned no results for: {query}")
    except Exception as e:
        print(f"   [WARN] Pexels error: {e}")
    return None


def render_cards(cardnews, analysis, figures_dir, output_dir):
    """HTML í…œí”Œë¦¿ + Playwrightë¡œ ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±"""
    from jinja2 import Environment, FileSystemLoader
    from playwright.sync_api import sync_playwright

    env = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)))
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1) Pexels ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ
    pexels_cache = {}
    for card in cardnews.get("cards", []):
        query = card.get("pexels_query", "")
        vs = card.get("visual_source", "")
        if vs == "pexels" and query and query not in pexels_cache:
            photo_path = output_dir / f"bg_{card['card_num']}.jpg"
            result = fetch_pexels_photo(query, str(photo_path))
            if result:
                pexels_cache[query] = str(photo_path)

    # 2) Playwrightë¡œ ë Œë”ë§
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page(viewport={"width": 1080, "height": 1080})

        for card in cardnews.get("cards", []):
            try:
                card_type = card.get("type", "cover")
                template_name = f"card_{card_type}.html"

                # ì´ë¯¸ì§€ ê²½ë¡œ ê²°ì •
                bg_image_path = ""
                figure_caption = card.get("figure_caption", "")
                chart_data = card.get("chart_data", {})

                vs = card.get("visual_source", "")
                if vs == "pexels":
                    query = card.get("pexels_query", "")
                    cached = pexels_cache.get(query)
                    if cached:
                        bg_image_path = f"file://{os.path.abspath(cached)}"
                elif vs == "paper_figure":
                    fig_file = card.get("figure_file", "")
                    if fig_file and figures_dir:
                        fig_path = Path(figures_dir) / fig_file
                        if fig_path.exists():
                            bg_image_path = f"file://{fig_path.absolute()}"

                # í…œí”Œë¦¿ ë Œë”ë§
                template = env.get_template(template_name)
                html = template.render(
                    bg_image_path=bg_image_path,
                    figure_caption=figure_caption,
                    chart_data=chart_data,
                    **{k: v for k, v in card.items()
                       if k not in ("visual_source", "pexels_query", "figure_file", "chart_data", "figure_caption")}
                )

                page.set_content(html, wait_until="networkidle")
                page.wait_for_timeout(800)  # ì´ë¯¸ì§€ ë¡œë”© ëŒ€ê¸°

                out_path = output_dir / f"card_{card['card_num']:02d}.png"
                page.screenshot(path=str(out_path))
                print(f"   ğŸ¨ Rendered: {out_path.name}")

            except Exception as e:
                print(f"   [WARN] Render error card {card.get('card_num')}: {e}")
                traceback.print_exc()

        browser.close()


# =============================================
# MAIN: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =============================================
def main():
    print("=" * 60)
    print(f"ğŸï¸ F1 Science Card News Generator â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    # STEP 1: ë…¼ë¬¸ ê²€ìƒ‰
    print("\nğŸ“š STEP 1: Searching papers...")
    papers = search_papers()
    if not papers:
        print("No new papers. Exiting.")
        return

    history = json.loads(HISTORY_FILE.read_text()) if HISTORY_FILE.exists() else []

    for paper in papers:
        doi = paper["doi"]
        title = paper.get("title", "Unknown")
        print(f"\n{'â”€' * 50}")
        print(f"ğŸ“„ Processing: {title}")
        print(f"   DOI: {doi}")

        try:
            # STEP 2: í…ìŠ¤íŠ¸ + Figure ì¶”ì¶œ
            print("\n   ğŸ“¥ STEP 2: Downloading & extracting...")
            text, figures, pdf_path = download_and_extract(paper)

            if not text.strip():
                print("   [SKIP] No text extracted.")
                continue

            figure_list_str = json.dumps(figures, indent=2) if figures else "ì—†ìŒ (ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ì´ë¯¸ì§€ ì—†ìŒ)"
            figures_dir = "/tmp/figures" if figures else None

            # Determine license (best effort)
            license_str = "Unknown (check paper)"
            oa_info = paper.get("openAccessPdf", {})
            if oa_info:
                license_str = "Open Access (likely CC-BY, verify on publisher site)"

            # STEP 3: ë…¼ë¬¸ ë¶„ì„
            print("\n   ğŸ”¬ STEP 3: Analyzing paper...")
            authors_str = ", ".join([
                a.get("name", "Unknown") for a in (paper.get("authors") or [])[:5]
            ])
            analysis_prompt = PROMPT_ANALYSIS.format(
                title=title, authors=authors_str, doi=doi,
                year=paper.get("year", "N/A"), venue=paper.get("venue", "N/A"),
                license=license_str, paper_text=text, figure_list=figure_list_str
            )
            analysis_raw = call_llm(analysis_prompt)
            analysis = parse_json_response(analysis_raw)
            print(f"   âœ… Analysis complete: {analysis.get('hook_headline', '?')}")

            time.sleep(3)

            # STEP 4: ì¹´ë“œë‰´ìŠ¤ ìŠ¤í¬ë¦½íŠ¸
            print("\n   âœï¸ STEP 4: Generating card news script...")
            use_figures = analysis.get("figure_selection", {}).get("use_paper_figures", False)
            available_figures = analysis.get("figure_selection", {}).get("selected_figures", [])

            cardnews_prompt = PROMPT_CARDNEWS.format(
                analysis_json=json.dumps(analysis, ensure_ascii=False, indent=2),
                use_figures=str(use_figures),
                available_figures=json.dumps(available_figures)
            )
            cardnews_raw = call_llm(cardnews_prompt)
            cardnews = parse_json_response(cardnews_raw)
            print(f"   âœ… Card script: {len(cardnews.get('cards', []))} cards generated")

            time.sleep(3)

            # STEP 5: ê²€ì¦
            print("\n   ğŸ” STEP 5: Verifying accuracy...")
            verify_prompt = PROMPT_VERIFY.format(
                paper_text_excerpt=text[:5000],
                license=license_str,
                analysis_json=json.dumps(analysis, ensure_ascii=False),
                cardnews_json=json.dumps(cardnews, ensure_ascii=False)
            )
            verify_raw = call_llm(verify_prompt)
            verification = parse_json_response(verify_raw)
            verdict = verification.get("verdict", "UNKNOWN")
            print(f"   âœ… Verification: {verdict}")

            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ìƒì„±
            if verdict == "REVISION_NEEDED":
                print("   ğŸ”„ Revision needed â€” regenerating card script...")
                revision_instructions = verification.get("revision_instructions", "")
                cardnews_prompt_v2 = PROMPT_CARDNEWS.format(
                    analysis_json=json.dumps(analysis, ensure_ascii=False, indent=2),
                    use_figures=str(use_figures),
                    available_figures=json.dumps(available_figures)
                ) + f"\n\n## ìˆ˜ì • ì§€ì‹œ (íŒ©íŠ¸ì²´ì»¤ í”¼ë“œë°±)\n{revision_instructions}"

                time.sleep(3)
                cardnews_raw_v2 = call_llm(cardnews_prompt_v2)
                cardnews = parse_json_response(cardnews_raw_v2)
                print(f"   âœ… Revised: {len(cardnews.get('cards', []))} cards")

            # STEP 6: ì´ë¯¸ì§€ ë Œë”ë§
            print("\n   ğŸ¨ STEP 6: Rendering card images...")
            date_str = datetime.now().strftime("%Y-%m-%d")
            safe_doi = doi.replace("/", "_").replace(".", "-")[:60]
            run_output_dir = OUTPUT_DIR / f"{date_str}_{safe_doi}"

            render_cards(cardnews, analysis, figures_dir, run_output_dir)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "paper": {"title": title, "doi": doi, "year": paper.get("year"),
                          "authors": authors_str, "venue": paper.get("venue")},
                "analysis": analysis,
                "cardnews": cardnews,
                "verification": verification,
                "generated_at": datetime.now().isoformat()
            }
            meta_path = run_output_dir / "metadata.json"
            meta_path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2))

            # Instagram caption ì €ì¥
            caption = cardnews.get("instagram_caption", "")
            if caption:
                (run_output_dir / "instagram_caption.txt").write_text(caption)

            # STEP 7: ì´ë ¥ ê°±ì‹ 
            history.append(doi)
            HISTORY_FILE.write_text(json.dumps(history, indent=2))

            print(f"\n   ğŸ DONE: {run_output_dir}")

        except Exception as e:
            print(f"\n   âŒ ERROR processing {doi}: {e}")
            traceback.print_exc()
            continue

        time.sleep(5)  # API rate limit ë°©ì§€

    print(f"\n{'=' * 60}")
    print("ğŸï¸ Pipeline complete!")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
